---
title: wget으로 사이트 미러링(복제)하기
categories: [프로그래밍,shell]
date: Sep 09, 2019 9:21 PM
permalink: wget-to-mirror-site
tags: [wget]
updated: Oct 05, 2019 12:30 PM
description: wget를 이용하여 사이트를 통째로 다운 받는 방법
---

예를들어 카카오 컨퍼런스([https://if.kakao.com](https://if.kakao.com) )의 내용이 공개되었을 경우, 해당 파일을 다운로드해서 태블릿등에서 보고 싶을때. 해당 페이지에서 클릭클릭해서 다운로드 하는 것 보다 통째로 저장하는 방법이 없을까 찾아봤더니, shell중 `wget`이 해당 기능을 쉽게 구현한다.

# `wget`으로 사이트 다운로드 받기

**긴 포멧 옵션**

    wget --recursive --level=inf --page-requisites --convert-links \
         --adjust-extension --span-hosts --domains=domainA,domainB domainA

**짧은 포멧 옵션**

    wget -rpkEH -l inf -D domainA,domainB domainA

- `-r` = `--recursive` 순환
- `-l <depth>` = `--level=<depth>` 링크 depth (`inf`나 `0`일 경우 무한)
- `-E` = `--adjust-extension`  HTML형태에 맞는 확장자로 저장
- `-p` = `--page-requisites` HTML에 맞는 이미지 포함
- `-K` = `--backup-converted` 변환된 파일에 대해 백업 파일을 생성
- `-k` = `--convert-links` 다운받은 HTML에 맞도록 링크 고정
- `-D <domain-list>` = `--domain-list=<domain-list>` 포함할 도메인 대상을 지정
- `--exclude-domains` to specify domains to be excluded
- `-H` = `--span-hosts`
- `-np` = `--no-parent` 상위로 올라가지 않음
- `-U <agent-string>` = `--user-agent=<agent-string>` 브라우저 agent 값을 지정

다음과 같이 수행하면 해당 디렉토리가 생성되며 관련 파일이 다운된다.

    # 기본 페이지 가져오기
    $ wget --recursive --level=inf --page-requisites --convert-links --html-extension if.kakao.com

그런데, `PDF`파일을 웠했던 것인데, 해당 파일은 다른 도메인이라 추가 다운로드가 안된다;; `--domain-list`옵션을 주긴 했는데 왠지 잘 작동하지 않음; 그래서 해당 도메인만 다운받도록 링크 필터

    # mk.kakaocdn.net 첨부파일들은 추출이 안되어서 별도 추출
    # program에서 href 링크를 추출
    $ cat if.kakao.com/program.html | grep -Eoi '<a [^>]+>' | grep -Eo 'href="[^\"]+"' | grep -Eo '(http|https)://[^"]+pdf' > 2019resource.list
    $ cat if.kakao.com/2018/program.html | grep -Eoi '<a [^>]+>' | grep -Eo 'href="[^\"]+"' | grep -Eo '(http|https)://mk.kakaocdn.net[^"]+pdf' >  2018resource.list

해당 파일은 대강 다음처럼 생김

    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T01-S01.pdf
    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T03-S01.pdf
    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T02-S01.pdf
    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T04-S01.pdf
    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T05-S01.pdf
    https://mk.kakaocdn.net/dn/if-kakao/conf2019/%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_2019/T01-S02.pdf

이걸 소스로 해서 다시 다운로드함. `wget`에 `-x` 옵션이 있을 경우 디렉토리 위치까지 강제 생성

    $ wget -x -i 2018resource.list
    $ wget -x -i 2019resource.list

마지막으로 html 파일들 내부의 링크를 변환하면 되는데... 그건 생략

이게 다운로드하는게 중요한게 아니라 보는게 중요한데... 틈나는대로 봐야함

# 참고

- Superuser : [Make wget download page resources on a different domain](https://superuser.com/questions/129085/make-wget-download-page-resources-on-a-different-domain)
- [wget mirror site + resources from other domain](https://unix.stackexchange.com/questions/263395/wget-mirror-site-resources-from-other-domain)
- [How to use grep and cut in script to obtain website URLs from an HTML file](https://unix.stackexchange.com/questions/181254/how-to-use-grep-and-cut-in-script-to-obtain-website-urls-from-an-html-file)
- [Scrape An Entire Website](https://stackoverflow.com/questions/9265172/scrape-an-entire-website)